%%% MPC Controller %%%

A feed-forward, inverse dynamics routine forms the basis for the control scheme at hand. 
Using the previously introduced notations, the inverse dynamics controller takes on the following general form:
	\begin{equation}
		\tau = \hat{M}(z_{1})[ \dot{z}_{2}^{r} + K_{1}(z_{1}^{r}-z_{1}) + K_{2}(z_{2}^{r}-z_{2}) ] + \hat{\Phi}
		\label{eq::inverse_dynamics}
	\end{equation}
where $\hat{M}(z_{1})$ and $\hat{\Phi}$ are approximations of $M$ and $\Phi$, respectively; and $K_{1}$ and $K_{2}$ are constant, square 
gain matrices. Since the disturbances generated during gaiting are present in the term $\Phi$ as a result of $f_{ext}$, and associated
propagation effects due to dynamical coupling, the control algorithm at hand will focus on learning its associated estimate, $\hat{\Phi}$.

In the controller at hand, the role of an estimator for $\hat{\Phi}$ is assumed by NARX-model neural network, trained on-line 
using the standard incremental back propagation (BP) algorithm with an adapted learning rate, $\gamma$  \cite{Rumelhart1988,Rumelhart1995}.

As previously mentioned, a network of this particular configuration has been chosen for its known effectiveness in modeling nonlinear difference 
equations and application to chaotic time-series prediction. Given that the NARX-network employs histories of presented input and output signals to 
make predictions, the success of this learning  mechanism is predicated on the of periodicity of the dynamics at hand, as the 
repetition of similar input and output sets is paramount for successful network training and, by extension, prediction stability. It is assumed 
that this specification can be met given the inherently cyclic nature of gaited locomotion. 
	\begin{figure}[t!]
		\centering
		\SetImage{\ImageWidthRatio}{narx_network_diagram.png}
		\caption{Parallel NARX-Network Model with a Linear Output layer}
		\label{fig::narx_net}
	\end{figure}
$\hat{\Phi}$ is generated by way of forward \emph{look-ahead} of the sampled system dynamics, $\psi_{k+1}$. 
The relationship between $\hat{\Phi}_{k}$, the estimate $\hat{\Phi}$ at time instance $k$,  and the network prediction $\hat{\psi}_{k+1}$
will be made clear in the description of the network training signal. 

The general input-output relationship of the NARX network predictor, $\mathscr{N}$, is described as follows:
	\begin{equation}
		\begin{split}
		\hat{\psi}_{k+1}&= \mathscr{N}(\hat{\Psi}_{k}^{N},{U}_{k}^{N}) \\
		{\Psi}_{k}^{N}	&= [\psi_{k},\psi_{k-1},...,\psi_{k-N+1}]  \\
		{U}_{k}^{N}		&= [u_{k}   ,u_{k-1}   ,...,u_{k-N+1}   ]
		\end{split}
		\label{eq::narx_model}
	\end{equation}
where $\hat{\Psi}_{k}^{N}$ and ${U}_{k}^{N}$ are collections of $N$ most recent samples of the network inputs and bouts, $\hat{\psi}$ and $u$, respectively. 
In this case each network  input entry, $u_{k}$, represents a tuple $u_{k} = (z_{1,k}, z_{2,k}, F_{ext,k})$ whose components are related to the 
arguments of the true $\Phi$ at time instance $k$. The mos prominent difference between these input sets that $F_{ext,k} \RealVec{12} $ is not equivalent to the
force-wrench $f_{ext}$. Instead, $F_{ext,k}$ is stacked vector of translational forces, $f_{i}\RealVec{3}$, applied to each \Ith end-effector, \IE 
$F_{ext} = [ f_{1}^{T}, f_{2}^{T}, f_{3}^{T}, f_{4}^{T} ]^T$. Here, we are more interested in learning a mapping between the variable distribution
of applied forces, which is subject to instantaneous, periodic changes. Additionally, sensing the translational force applied to each foot is much
more straightforward in an, implementation sense, then sensing the full force-wrench, which also contains a torsional component.


\subsection{Training the NARX Network}
	

A formulation for the  NARX network training signal for the begins by isolating the unknown portions of the system dynamics, $\Phi_{k}$, 
which can be estimated using predictions which incorporate the system's state at time instance $k+1$, namely ${z}_{2,k+1}$. This look ahead is made 
possible by the predictive capabilities of the NARX network.
	\begin{equation}
		\Psi_{k+1} = \tau_{k} - \hat{M}_{1,k}({z}_{2,k+1} - {z}_{2,k})\Delta_{s}^{-1} = \Phi_{k} - {e}_{2,k}^{\Delta_{s}} \\
		\label{eq::training_signal}
	\end{equation}
It should be noted that in addition to learning to predict the system dynamics, the training signal at hand teaches the network to absolve
the discretization error, ${e}_{2,k}^{\Delta_{s}}$, when applied to a controller based on the discrete time model described by (\ref{eq::sampled_dynamics}).

This formulation (\ref{eq::training_signal}) assumes that $\hat{M}_{1,k}$ represents $M_{1,k}$ to some exactness, which is likely not the case given the system's complexity. 
In the absence of $\hat{M}_{1,k}$ in a more accurate form, a constant symmetric $\hat{M}_{nom}$ will be picked such that $\hat{M}_{1,k} = \hat{M}_{nom} \forall k$. 
$\hat{M}_{nom}$ has the following structure:
	\begin{equation}
		\hat{M}_{nom} = \left[
			\begin{array}{cc}
			\hat{M}_{bb}	&	 \hat{M}_{bq}\\
			\hat{M}_{qb}	&	 \hat{M}_{qq}
			\end{array}
		\right]
	\end{equation}
where $\hat{M}_{nom}\RealMat{(4m+6)}{(4m+6)}$ is a symmetric, constant matrix with component blocks $\hat{M}_{bb}\RealMat{6}{6}$, 
$\hat{M}_{bq}=\hat{M}_{qb}^{T} \RealMat{6}{(4m)}$, and  $\hat{M}_{qq}\RealMat{(4m)}{(4m)}$. It is particularly important that $M_{bq}\neq0$ 
to reflect some degree of coupling between the joint states $q$ and the trunk states $p_{b}$ and $\theta_{b}$. In general, $\hat{M}_{nom}$ should
be selected to reflect the \emph{average} system mass matrix over the range of configurations, $z_{1}$, seen during gaiting. 

To train the predictor, the network must wait until time instance $(k+1)$ when a ``true" value for $\hat{\psi}_{k+1}$, becomes known.
Since $\psi_{k}$ is a completely causal signal, it can be calculated directly. The network is, then, trained using the set of previous input, \{ ${\Psi}_{k-1}^{N}$,
${U}_{k-1}^{N}$\}, and the true output training signal, $\psi_{k}$, as follows:
	\begin{equation}
		\psi_{k} \overset{BP_{\gamma}}{\rightarrow} \mathscr{N}({\Psi}_{k-1}^{N},{U}_{k-1}^{N})
		\label{eq::training}
	\end{equation}
where $\gamma$ is a learning rate adapted using a \emph{bold-driver} update routine, parameterized by $\beta \in (0,1)$ and  $\zeta \in (0,1)$, which represent
an exponential learning rate modification and penalization-bias factors, respectively. 
This scheme is heuristic methods for speeding up the rate of convergence of back-propagation training regimes \cite{Battiti1992,Magoulas1999}. 
The bold driver routine updates $\gamma$ according to the mean-squared output error values $(MSE)_{k}$ and $(MSE)_{k-1}$ as follows:
	\begin{equation}
	    \gamma \leftarrow 
		\begin{cases}
	    \gamma (1- \beta) 		& \text{if } (MSE)_{k} > (MSE)_{k-1}\\
	    \gamma (1+\zeta \beta),& \text{otherwise}
		\end{cases}
	\end{equation}
In cases where network training is being performed on-line as an incremental routine, the effective mean-squared error of the network output low-passed
by a factor $\lambda \in (0,1)$. This is performed so as to ensure that outliers presented during training do not effect network learning rate updates as 
significantly. Moreover, network output error, $e_{\mathscr{N},k}$ and its associated MSE values are calculated one time instance after each prediction is 
made by:
	\begin{equation}
		\begin{split}
		e_{\mathscr{N},k} 	&= \hat{\psi}_{k} - \psi_{k} \\
		(MSE)_{k} 			&= \lambda \|e_{\mathscr{N},k}\|_{2}^{2} + (MSE)_{k-1}(\lambda-1)
		\end{split}
	\end{equation}


\subsection{Compensator Outputs}


The controller scheme is first presented with respect to the general input torques, $\tau_{k}$ and is based on a discrete-time 
representation of an inverse dynamics controller. Instead of using an approximate dynamical model for the system, unknown system dynamics are 
substituted with the $\hat{\Psi}_{k+1}$ generated as the output of the network, $\mathscr{N}_{k}$.
respectively.
	\begin{equation}
		\begin{split}
		%\mathscr{N}_{k} 	&= \hat{\Psi}_{k+1} = \hat{\Phi}_{k} - {e}_{2,k}^{\Delta_{s}} \\
		\tau_{k}   			&=  M_{1,k}[ \Delta e_{2,k+1} + K_{p} e_{1,k} + K_{p} e_{2,k}] + \hat{\Psi}_{k+1} \\
			e_{1,k}			&= {z}_{1,k}^{r} - {z}_{1,k} \\
			e_{2,k}			&= {z}_{2,k}^{r} - {z}_{2,k} \\
			\Delta e_{2,k+1}&= ({z}_{2,k+1}^{r} - {z}_{2,k})\Delta_{s}^{-1}\\
		\end{split}
		\label{eq::digital_controller_base}
	\end{equation}
where ${z}_{2,k+1}^{r}$ defines the velocity commands issued to each states during pre-planned gaiting sequence. 

The control scheme input, $\tau_{k}$, must now be tailored to the available system control inputs, $\tau_{q}$. Given the actuator model 
(\ref{eq::servo_control_dynamics}), we will assumed that torques are controlled indirectly via supplied actuator reference positions, and will instead consider 
an outer loop controller for generating a set of ``corrected'' actuator positions ${z}_{k}^{r,*}$.
	\begin{equation}
	 	{z}_{1,k}^{r,*} 	= [K_{s}]^{\dagger} \left[ M_{1,k}[ \Delta e_{2,k+1} + K_{p} e_{1,k} + K_{d} e_{2,k}] + \hat{\Psi}_{k+1} \right] +  {z}_{1,k}
		\label{eq::correction_equation}
	\end{equation}
where the $[K_{s}]^{\dagger}$ is the Penrose-Moore pseudo-inverse of the $K_{s}$. Given the structure of $K_{s}$, \label{eq::correction_equation} the 
compensator output ${z}_{1,k}^{r,*}$ is reduced to ${z}_{1,k}^{r,*} = [ 0_{1\times6}, ({q}_{1,k}^{r,*})^{T} ]^T$.
	\begin{figure}[b!]
		\centering
		\SetImage{\ImageWidthRatio}{controller_diagram.png}
		\caption{Full system diagram with NARX-Compensator Mechanism}
		\label{fig::sys_diagram}
	\end{figure}
Here is it assumed that the states $\dot{p}_{b}$ and $\dot{\theta}_{b}$ are bounded and the sub-system which governs the trunk-state 
dynamics is asymptotically stable provided that the all joint states are bounded during gaiting. Additionally, it is assumed all external forces 
which disturb the trunk states enter the system through the legs, \IE there no external forces being applied to the body states. Thus, it is sensible
to perform the necessary disturbance rejection control in the joint-space.


The correction signal,  ${q}_{k}^{r,*}$ is combined with the original trajectory signal ${q}_{k}^{r}$ as follows:
	\begin{equation}
	 	\tilde{q}_{k}^{r} 	\leftarrow (1-\alpha) {q}_{k}^{r} + \alpha ( {q}_{k}^{r,*} )
		\label{eq::correction_application}
	\end{equation}
where  $\alpha \in (0,1)$ is a mixing parameter for combining the \emph{a priori} reference trajectory vector ${q}_{k}^{r}$ and the correction 
signal ${q}_{k}^{r,*}$.